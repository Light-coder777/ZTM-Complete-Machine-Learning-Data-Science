# -*- coding: utf-8 -*-
"""dog-vision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mNMfgiO_7CSGu1E4vB9WUsmoADbkR-Nt

# 🐶 End-to-end Multi-Class Dog Breed Classification

This notebook builds an end-to-end multiclass image classifier using Tensorflow 2 and Tensorflow Hub.

## 1. Problem

Identifying the breed of a dog given an image of a dog.

## 2. Data

The data comes from Kaggle's Dog Breed Identification competition: [Kaggle](https://www.kaggle.com/c/dog-breed-identification/).

## 3. Evaluation

The evaluation is a file with prediction probabilities forEach dog breed for each test image: [Evaluation](https://www.kaggle.com/c/dog-breed-identification/overview/evaluation).


## 4. Features

Some information about the data:
* images = unstructured data -> it's probably best to use deep learning/transfer learning
* 120 breeds of dogs (multi-class)
* there 10,000+ images in the training set (with labels)
* there are 10,000+ images in the testing set (no labels)

## 0. Get Workspace Ready

* import Tensorflow 2.x
* import Tensorflow Hub
* make sure we're using a GPU
"""

# Commented out IPython magic to ensure Python compatibility.
# import Tensorflow TF 2.0
try:
  # %tensorflow_version only exists in Colab
#   %tensorflow_version 2.x
except Exception:
  pass

# import necessary tools
import tensorflow as tf
import tensorflow_hub as hub
print('TF version:', tf.__version__)
print('TF Hub version:', hub.__version__)

# Check for GPU available
print('GPU', 'available (Yeah!)' if tf.config.list_physical_devices('GPU') else 'not avaiable...')

"""### Getting Data Ready

Transform input data into numerical format (Tensors).

Let's start by accessing our data and checking out labels
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns; sns.set()

labels_csv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/dog-breed/labels.csv')
labels_csv.describe()

# How images are there for each breed?
labels_csv['breed'].value_counts()

# Plotting a bar chart for the target values
breed_count = labels_csv['breed'].value_counts()
plt.figure(figsize=(12,8))
sns.barplot(breed_count.index, breed_count.values,
            order=breed_count.index,
            alpha=0.8)
plt.title('Number of Images per Breed', fontdict={'fontsize': 16})
plt.ylabel('Number of Images')
plt.xlabel('Breeds')
plt.xticks([]);

# Average number of images per breed
breed_count.median()

# Let's view an image
from IPython.display import Image
Image('/content/drive/My Drive/Colab Notebooks/data/dog-breed/train/0a0c223352985ec154fd604d7ddceabd.jpg')

"""### Getting images and their labels

Let's get a list of all image file pathnames.
"""

labels_csv.head()

# Create pathnames from image IDs
fpath = '/content/drive/My Drive/Colab Notebooks/data/dog-breed/train/'
filenames = [fpath + fname + '.jpg' for fname in labels_csv['id']]
filenames[:5]

# Check if number of filenames matches number of actual image files
import os
if len(os.listdir(fpath)) == len(filenames):
  print('Filenames match actual amount of files. Proceed.')
else:
  print(f'Error: actual num of files: {len(os.listdir(fpath))} - labels.csv: {len(filenames)}')

# Another check
Image(filenames[9000])

"""The training image filepaths are now accessible as a Python list.
Now prepare the labels.
"""

labels = np.array(labels_csv['breed'])
labels

len(labels)

# Check if number of labels matches number of filenames
len(filenames) == len(labels)

unique_breeds = np.unique(labels)

# Check number of unique breeds (should be 120)
len(unique_breeds) == 120

"""**We need to encode the labels**. The labels are categorical data with nominal values (no intrinsic order). See [Categorical Variables](https://github.com/sophiabrandt/udemy-feature-engineering/blob/master/notebooks/original-course-notebooks/Section-02-Types-of-Variables/02.2-Categorical-Variables.ipynb)."""

# Use LabelBinarizer to encode labels
from sklearn.preprocessing import LabelBinarizer

# Create one-hot encoder
one_hot = LabelBinarizer()

# One-hot encode labels
encoded_labels = one_hot.fit_transform(labels)
encoded_labels[0]

"""### Creating our own validation set

Since the dataset from Kaggle doesn't come with a validation set, we're going to create our own.
"""

# Setup X & y variables
X = filenames
y = encoded_labels

"""We're going to start off experimentind with ~1000 images for a fast feedback loop and increase as needed."""

# Set number of images to use for experimenting
NUM_IMAGES = 1000 # @param {type: 'slider', min:1000, max: 10222}

# Let's split our data into train and validation sets
from sklearn.model_selection import train_test_split

# Split data into training and validation of total size NUM_IMAGES
X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],
                                                  y[:NUM_IMAGES],
                                                  test_size=0.2,
                                                  random_state=33)

len(X_train), len(y_train), len(X_val), len(y_val)

# Check the training data
X_train[:2], y_train[:2]

"""## Preprocessing Images (Turning Images into Tensors)

We'll write a preprocess function that does the following:

1. Take an image filepath as input (string)
2. Use Tensorflow to read the file and save it to variable `image`
3. Turn `image` (a jpg) into Tensors
4. Normalize image (convert color channel values from 0-255 to 0-1)
5. Resize `image` to be a shape of (224, 224)
6. Return the modified `image`
"""

# Prior step: turn image into Numpy array
from matplotlib.pyplot import imread
image = imread(filenames[3])
image.shape

image

tf.constant(image)

# Define image size
IMG_SIZE = 224

# Create a function for preprocessing images
def process_image(image_path, img_size=IMG_SIZE):
  """
  Takes an image file path (string) and turns it into a Tensor.
  """
  # Read the image file
  image = tf.io.read_file(image_path)
  # Turn the jpg image into numerical Tensor with 3 color channels
  image = tf.image.decode_jpeg(image, channels=3)
  # Convert  color channel values from 0-255 to 0-1 values
  image = tf.image.convert_image_dtype(image, dtype=tf.float32)
  # Resize image to (224, 244)
  image = tf.image.resize(image, size=[img_size, img_size])

  return image

"""## Turning data into batches

Why turn our data into batches?

Large amounts of data might not fit into memory.

We need to limit our images to batch-size. We start with 32 images per run.

To use Tensorflow effectively, the data must be in the form of Tensor tuples.
"""

# Create a function that returns a tuple (image, label)
def to_tf_tuple(image_path, label):
  """
  Takes an image filepath (string) and associated label (numpy array),
  preproccesses the image,
  and returns a tuple of (image, label):
  """
  image = process_image(image_path)
  return image, label

to_tf_tuple(X_train[0], y_train[0])

"""Now let's make a function to turn all our data into batches of Tensor tuples."""

# Define batch size
BATCH_SIZE = 32

# Helper function to create a Tensor dataset
def to_tf_dataset(X, y=None):
  """
  Returns a Tensor Dataset from Tensor slices.
  """
  dataset = tf.data.Dataset.from_tensor_slices((X, y))
  return dataset

# Create a function to turn data into btches
def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  """
  Creates batches of size `batch_size` out of image (X) and
  one-hot encoded label (y) pairs.
  Shuffles the data if it's training data but doesn't shuffle validation data.
  Also accepts test data as input (no labels).
  """
  # For test data, batch data without labels
  if test_data:
    print('Creating test data batches ...')
    data = to_tf_dataset(tf.constant(X)) # only file paths, no labels
    data_batch = data.map(process_image).batch(BATCH_SIZE)
    return data_batch

  # For validation dataset, batch data, with labels, without shuffling
  elif valid_data:
    print('Creating validation data batches ...')
    data = to_tf_dataset(tf.constant(X),  # file paths
                          tf.constant(y)) # labels 
    data_batch = data.map(to_tf_tuple).batch(BATCH_SIZE)
    return data_batch

  # For training data, shuffle and batch, with labels
  else:
    print('Creating training data batches...')
    # Turn filepaths and labels into Tensors
    data = to_tf_dataset(tf.constant(X),
                          tf.constant(y))
    # Shuffling pathnames and labels
    data = data.shuffle(buffer_size=len(X))
    # Create (image, label) tuples
    data = data.map(to_tf_tuple)
    # Turn training data into batches
    data_batch = data.batch(BATCH_SIZE)

    return data_batch

# Creating and validation data batches
train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val, y_val, valid_data=True)

# Check out the different attributes of the data batches
train_data.element_spec, val_data.element_spec

"""## Visualizing Data Batches

The data is now in batches. But they are hard to understand. Let's visualize them!
"""

# Create a funtion for viewing images in a data batch

def show_25_images(images, labels):
  """
  Displays a plot of 25 images
  and their labels from a data batch.
  """
  plt.figure(figsize=(10,10))
  # Loop until 25
  for i in range(25):
    # Create subplots
    ax = plt.subplot(5, 5, i+1)
    # Display an image
    plt.imshow(images[i])
    # Add the image label
    plt.title(unique_breeds[labels[i].argmax()])
    # Turn the grid lines off
    plt.axis('off')

train_images, train_labels = next(train_data.as_numpy_iterator())

train_data, train_labels

# Let's visualize the data in a training batch
show_25_images(train_images, train_labels)

# Visualize data in validation batch
val_images, val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

"""## Buiding a Model

Before we build a model, there are a few things we need to define:

* The _input_ shape (images shape, in the form of Tensors)
* The _output_ shape (image labels, in the form of Tensors)
* The URL of the Tensorflow Hub model we want to use.
"""

# Setup input shape to the model
INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, color channels

# Setup the output shape
OUTPUT_SHAPE = len(unique_breeds)

# Setup model URL from Tensorflow Hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"

"""Let's put everything together into a Keras deep learning model.

We create a function which:

* takes the input shape, output shape and the model as parameters
* defines the layers in a Keras model in sequential fashion
* compiles the model (it should be evaluated andImproved)
* builds the model (tells the model the input shape)
* returns the model

For further information, see [Keras overview](https://www.tensorflow.org/guide/keras/overview).
"""

# Create a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE,
                 output_shape=OUTPUT_SHAPE,
                 model_url=MODEL_URL):
  """
  Creates a Keras model from Tensorflow Hub.
  """
  print('Building model with: ', MODEL_URL)
  
  # Setup the model
  model = tf.keras.Sequential([
                               hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)
                               tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                                                     activation='softmax') # Layer 2 (output layer)
  ])

  # Compile the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=['accuracy']
  )

  # Build the model
  model.build(INPUT_SHAPE)

  return model

model = create_model()
model.summary()

"""## Creating callbacks

Callbacks are helper functions a model can use during training to save its progress, check its progress or stop training if the model stops improving.

We'll create two callbacks, one for **TensorBoard** (tracks the model's grogress), one for **early stopping** (prevents model from over-fitting).


### TensorBoard Callback

Three steps:

1. Load Tensorboard notebook extension
2. Create a Tensorboard callback which is able to save logs to a directory and pass it to the model's `fit()` function
3. Visualize model's training logs with `%tensorboard` magic function
"""

# Commented out IPython magic to ensure Python compatibility.
# Load Tensorboard
# %load_ext tensorboard

import datetime

# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log directory for storing Tensorboard logs
  log_dir = os.path.joins('drive/My Drive/Colab Notebooks/dog-breeds/data/logs',
                         # Logs get tracked when we run an experiment
                         datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
  return tf.keras.callbacks.TensorBoard(log_dir=log_dir)

"""#### Early Stopping Callback

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping

Early stopping hinders theModel from overfitting if a particular evaluation metric stops improving.
"""

